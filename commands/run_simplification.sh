DEBUG_params="--num_inference_diffusion_steps 10 --max_eval_samples 10 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --eval_steps 10"
PARAMS_FOR_LOCAL=" --save_total_limit 1 "


# debug
# python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name cc  --output_dir /net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/debug --per_device_train_batch_size=12 --per_device_eval_batch_size=64 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps 2 --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --fp16 --self_condition "logits_mean" --eval_steps 10 --max_eval_samples 64  --dataset_folder "/net/nfs.cirrascale/s2-research/rabeehk/simplex-diffusion/datasets/cc/"

# data length=512
: '
num_inference_diffusion_steps=600
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name asset  --output_dir /net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_${num_inference_diffusion_steps} --per_device_train_batch_size=12 --per_device_eval_batch_size=15 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_eval_samples 96 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --fp16 --self_condition "logits_mean"
'

: '
# Running on all the data.
num_inference_diffusion_steps=200
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name asset  --output_dir /net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_${num_inference_diffusion_steps}_all_data --per_device_train_batch_size=12 --per_device_eval_batch_size=15 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --fp16 --self_condition "logits_mean"
'

######################################################
# self-conditioning with different setting.
######################################################
num_inference_diffusion_steps=1000
: '
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_max" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_max"

python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_mean" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_mean"
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_addition" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_addition"


python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_multiply" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_multiply"


python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_max_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_max"  --self_condition_mix_before_weights true
'

: '
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_mean_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25  --report_to tensorboard --eval_steps 1000  --max_steps 18000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_mean"  --self_condition_mix_before_weights true  --resume_from_checkpoint "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean_self_condition_mix_before_weights/checkpoint-17000/"
'

: '
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_addition_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_addition"  --self_condition_mix_before_weights true
'

: '
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_multiply_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25  --report_to tensorboard --eval_steps 1000  --max_steps 18000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_multiply"  --self_condition_mix_before_weights true --resume_from_checkpoint /net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply_self_condition_mix_before_weights/checkpoint-17000/
'

: '
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/no_self_condition" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100
'

#python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_max_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25  --report_to tensorboard --eval_steps 1000  --max_steps 18000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_max"  --self_condition_mix_logits_before_weights true --resume_from_checkpoint "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max_self_condition_mix_logits_before_weights/checkpoint-14000/"

#python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_mean_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25  --report_to tensorboard --eval_steps 1000  --max_steps 18000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_mean"  --self_condition_mix_logits_before_weights true --resume_from_checkpoint /net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean_self_condition_mix_logits_before_weights/checkpoint-14000/

#python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_addition_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 18000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_addition"  --self_condition_mix_logits_before_weights true --resume_from_checkpoint "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition_self_condition_mix_logits_before_weights/checkpoint-14000/"

# python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_multiply_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25  --report_to tensorboard --eval_steps 1000  --max_steps 18000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_multiply"  --self_condition_mix_logits_before_weights true --resume_from_checkpoint "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply_self_condition_mix_logits_before_weights/checkpoint-16000/"

######################################################
# python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/debug/" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100  --self_condition_mix_logits_before_weights true --self_condition "logits_mean"
######################################################

# Running the evals.
num_inference_diffusion_steps=1000
: '
python -m torch.distributed.launch --nproc_per_node 4  run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max/checkpoint-19000/"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_max" --load_states_in_eval_from_model_path true
python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean/checkpoint-19000"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --self_condition "logits_mean" --load_states_in_eval_from_model_path true
python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition/checkpoint-19000"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition" --per_device_train_batch_size=12 --per_device_eval_batch_size=25  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_addition" --load_states_in_eval_from_model_path true
python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply/checkpoint-18000"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_multiply" --load_states_in_eval_from_model_path true
'

: '
python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max_self_condition_mix_before_weights/checkpoint-18000" --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --self_condition "logits_max"  --self_condition_mix_before_weights true  --load_states_in_eval_from_model_path true
'
#python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean_self_condition_mix_before_weights/checkpoint-18000"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true

#python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition_self_condition_mix_before_weights/checkpoint-18000/"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --self_condition "logits_addition"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true

#python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply_self_condition_mix_before_weights/checkpoint-18000"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_multiply"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true

# python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/no_self_condition/checkpoint-22000"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/no_self_condition" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --load_states_in_eval_from_model_path

: '
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max_self_condition_mix_logits_before_weights/checkpoint-18000"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --self_condition "logits_max"  --self_condition_mix_logits_before_weights true --load_states_in_eval_from_model_path true
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean_self_condition_mix_logits_before_weights/checkpoint-18000"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_mean"  --self_condition_mix_logits_before_weights true --load_states_in_eval_from_model_path true
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition_self_condition_mix_logits_before_weights/checkpoint-18000" --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_addition"  --self_condition_mix_logits_before_weights true --load_states_in_eval_from_model_path true
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply_self_condition_mix_logits_before_weights/checkpoint-18000"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_multiply"  --self_condition_mix_logits_before_weights true --load_states_in_eval_from_model_path true
'

#############################################################
# Tune different lrs with the best self-conditioing model
#############################################################
# NOTE: batch size is divided by two.
learning_rate=5e-5 #, 5e-5, 1e-5, 2e-5
num_inference_diffusion_steps=1000
# python -m torch.distributed.launch --nproc_per_node 2 run_simplification.py --model_name_or_path "roberta-large" --do_train --do_eval --dataset_name wikilarge  --output_dir  "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/tune_lrs_simplification/lr_"${learning_rate} --per_device_train_batch_size=6 --per_device_eval_batch_size=24   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000  --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 --resume_from_checkpoint  /net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/tune_lrs_simplification/lr_5e-5/checkpoint-72000/ ${PARAMS_FOR_LOCAL}

#python -m torch.distributed.launch --nproc_per_node 2 run_simplification.py --model_name_or_path "roberta-large" --do_train --do_eval --dataset_name wikilarge  --output_dir  "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/tune_lrs_simplification/lr_"${learning_rate}"_no_wd" --per_device_train_batch_size=6 --per_device_eval_batch_size=24   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96  --resume_from_checkpoint /net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/tune_lrs_simplification/lr_5e-5_no_wd/checkpoint-74000/ ${PARAMS_FOR_LOCAL}

#############################################################
# *** Train the best model ***
# lr=3e-5 and 2e-5 can be the best.
learning_rate=2e-5 
num_inference_diffusion_steps=1000
# python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-large" --do_train --do_eval --dataset_name wikilarge  --output_dir  "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_results/ours_lr_"${learning_rate}"_no_wd" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 500000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "wikilarge"

: '
for learning_rate in 2e-5
do
  for TOP_P in 0.9 0.95 0.99	  
  do
	  for TEMPERATURE in 1 2 4
	  do  
          model_name="simplification_results/ours_2e-5/checkpoint-400000"
          num_inference_diffusion_steps=1000
          python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_name} --do_predict --do_eval --dataset_name wikilarge  --output_dir  "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_results/ours_lr_"${learning_rate}"_no_wd" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 500000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p ${TOP_P} --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "wikilarge" --temperature ${TEMPERATURE} --load_states_in_eval_from_model_path true
  done
  done
done 
# evaluate for top-p=None
learning_rate=2e-5
for TEMPERATURE in 1 2 4
do  
    model_name="simplification_results/ours_2e-5/checkpoint-400000"
    num_inference_diffusion_steps=1000
    python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_name} --do_predict --do_eval --dataset_name wikilarge  --output_dir  "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_results/ours_lr_"${learning_rate}"_no_wd" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 500000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "wikilarge" --temperature ${TEMPERATURE} --load_states_in_eval_from_model_path true
done
'



##########################################################
# Ablation for the self-conditioning methods.
##########################################################
# TODO: run this.
learning_rate=2e-5 
num_inference_diffusion_steps=1000
# python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-large" --do_train --do_eval --dataset_name wikilarge  --output_dir  "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/self_condition_ablation_results/simplification_lr_"${learning_rate}"_no_wd_logits_mean_mix_before_weights" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 20000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean_self_condition_mix_before_weights"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3  --dataset_folder "wikilarge"

# python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-large" --do_train --do_eval --dataset_name wikilarge  --output_dir  "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/self_condition_ablation_results/simplification_lr_"${learning_rate}"_no_wd_logits" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 20000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits"   --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3  --dataset_folder "wikilarge"
##########################################################
# DEBUG
#python run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --dataset_name wiki_alignment  --output_dir  "/net/nfs.cirrascale/s2-research/rabeehk/outputs/debug" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 20000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits"   --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0  --dataset_folder "/net/nfs.cirrascale/s2-research/rabeehk/simplex-diffusion/datasets/wiki_alignment/" --overwrite_output_dir --eval_steps 2  --num_inference_diffusion_steps 10

##########################################################
# Training on wiki_alignment.
# ****** this is selected *******
learning_rate=3e-5 
num_inference_diffusion_steps=1000
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/wiki_alignment_tune_lr/ours_lr_"${learning_rate}"_no_wd" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "/net/nfs.cirrascale/s2-research/rabeehk/simplex-diffusion/datasets/wiki_alignment/"

# Train the wiki-alignment base model with classifier-free guidance.
guidance=2.0
learning_rate=3e-5 
num_inference_diffusion_steps=1000
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/wiki_alignment_with_guidance/ours_lr_"${learning_rate}"_no_wd_guidance_"${guidance} --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 10000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 10000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "/net/nfs.cirrascale/s2-research/rabeehk/simplex-diffusion/datasets/wiki_alignment/" --guidance_scale ${guidance}



: '
# Evaluate the above model.
for learning_rate in 3e-5 #2e-5 3e-5 5e-5
do
  for TOP_P in 0.9 0.95 0.99	  
  do
	  for TEMPERATURE in 1  #2 4
	  do  
model_name="/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/wiki_alignment_tune_lr/ours_lr_"${learning_rate}"_no_wd/checkpoint-80000"
python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_name} --do_eval  --do_predict --dataset_name wiki_alignment  --output_dir  "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/wiki_alignment_tune_lr/ours_lr_"${learning_rate}"_no_wd" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p ${TOP_P} --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "/net/nfs.cirrascale/s2-research/rabeehk/simplex-diffusion/datasets/wiki_alignment/" --load_states_in_eval_from_model_path true --temperature ${TEMPERATURE}
  done
  
  # top-p = None
  TEMPERATURE=1
  model_name="/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/wiki_alignment_tune_lr/ours_lr_"${learning_rate}"_no_wd/checkpoint-80000"
  python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_name} --do_eval  --do_predict --dataset_name wiki_alignment  --output_dir  "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/wiki_alignment_tune_lr/ours_lr_"${learning_rate}"_no_wd" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "/net/nfs.cirrascale/s2-research/rabeehk/simplex-diffusion/datasets/wiki_alignment/" --load_states_in_eval_from_model_path true --temperature ${TEMPERATURE}
  done
done  
'


# Set more max-steps and iters => did not worked.
# learning_rate=3e-5 
# num_inference_diffusion_steps=1000
# max_steps=100000
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --dataset_name wiki_alignment  --output_dir  "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/wiki_alignment_tune_lr_steps/ours_lr_"${learning_rate}"_no_wd_steps_"${max_steps} --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 20000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "/net/nfs.cirrascale/s2-research/rabeehk/simplex-diffusion/datasets/wiki_alignment/"

# Run the evaluation of the above model.
#learning_rate=1e-5 
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "checkpoint-35000/" --do_predict --dataset_name wiki_alignment  --output_dir  "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/wiki_alignment_tune_lr/ours_lr_"${learning_rate}"_no_wd" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 1000  --max_steps 80000 --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "/net/nfs.cirrascale/s2-research/rabeehk/simplex-diffusion/datasets/wiki_alignment/"
######################################################
# Ablation for the self-condition.
learning_rate=3e-5 
num_inference_diffusion_steps=1000
max_steps=60000
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/wiki_alignment_self_condition_ablations/lr_"${learning_rate}"_no_wd_steps_"${max_steps}"_self_condition_logits_mean_ours_mix_before_weights" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 20000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "/net/nfs.cirrascale/s2-research/rabeehk/simplex-diffusion/datasets/wiki_alignment/"


#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/wiki_alignment_self_condition_ablations/lr_"${learning_rate}"_no_wd_steps_"${max_steps}"_self_condition_logits" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 20000    --self_condition "logits" --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "/net/nfs.cirrascale/s2-research/rabeehk/simplex-diffusion/datasets/wiki_alignment/"


#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path "roberta-base" --do_train --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/wiki_alignment_self_condition_ablations/lr_"${learning_rate}"_no_wd_steps_"${max_steps}"_no_self_condition" --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 20000   --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "/net/nfs.cirrascale/s2-research/rabeehk/simplex-diffusion/datasets/wiki_alignment/"

# run the eval for top-p=None
#model_path="/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/wiki_alignment_self_condition_ablations/lr_"${learning_rate}"_no_wd_steps_"${max_steps}"_self_condition_logits_mean_ours_mix_before_weights/checkpoint-60000"
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path} --do_eval --do_predict --dataset_name wiki_alignment  --output_dir ${model_path} --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --warmup_steps 2000 --logging_steps 50 --save_steps 20000    --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "/net/nfs.cirrascale/s2-research/rabeehk/simplex-diffusion/datasets/wiki_alignment/"

#model_path="/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/wiki_alignment_self_condition_ablations/lr_"${learning_rate}"_no_wd_steps_"${max_steps}"_self_condition_logits/checkpoint-60000"
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  ${model_path} --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 20000    --self_condition "logits" --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "/net/nfs.cirrascale/s2-research/rabeehk/simplex-diffusion/datasets/wiki_alignment/"

#model_path="/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/wiki_alignment_self_condition_ablations/lr_"${learning_rate}"_no_wd_steps_"${max_steps}"_no_self_condition/checkpoint-60000"
#python -m torch.distributed.launch --nproc_per_node 8 run_simplification.py --model_name_or_path ${model_path}  --do_eval --do_predict --dataset_name wiki_alignment  --output_dir  ${model_path} --per_device_train_batch_size=1 --per_device_eval_batch_size=12   --report_to tensorboard --eval_steps 20000  --max_steps ${max_steps} --max_source_length 128  --max_target_length 128 --max_seq_length 256 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate ${learning_rate} --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm  --warmup_steps 2000 --logging_steps 50 --save_steps 20000   --load_states_in_eval_from_model_path true --max_eval_samples 96 ${PARAMS_FOR_LOCAL} --weight_decay 0.0 --save_checkpoints_on_s3 --dataset_folder "/net/nfs.cirrascale/s2-research/rabeehk/simplex-diffusion/datasets/wiki_alignment/"


