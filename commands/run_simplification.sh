DEBUG_params="--num_inference_diffusion_steps 10 --max_eval_samples 10 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --eval_steps 10"
PARAMS_FOR_LOCAL=" --save_total_limit 1 "


: '
# debug
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name asset  --output_dir /net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/debug --per_device_train_batch_size=12 --per_device_eval_batch_size=64 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps 200 --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --fp16 --self_condition "logits_mean" --eval_steps 10 --max_eval_samples 64
'

# data length=512
: '
num_inference_diffusion_steps=600
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name asset  --output_dir /net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_${num_inference_diffusion_steps} --per_device_train_batch_size=12 --per_device_eval_batch_size=15 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_eval_samples 96 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --fp16 --self_condition "logits_mean"
'

: '
# Running on all the data.
num_inference_diffusion_steps=200
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name asset  --output_dir /net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_${num_inference_diffusion_steps}_all_data --per_device_train_batch_size=12 --per_device_eval_batch_size=15 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --fp16 --self_condition "logits_mean"
'

######################################################
# self-conditioning with different setting.
######################################################
num_inference_diffusion_steps=1000
: '
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_max" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_max"

python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_mean" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_mean"
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_addition" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_addition"


python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_multiply" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_multiply"


python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_max_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_max"  --self_condition_mix_before_weights true


python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_mean_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_mean"  --self_condition_mix_before_weights true

python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_addition_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_addition"  --self_condition_mix_before_weights true

python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_multiply_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_multiply"  --self_condition_mix_before_weights true

'
: '
python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/no_self_condition" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100
'

#python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_max_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_max"  --self_condition_mix_logits_before_weights true
#python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_mean_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_mean"  --self_condition_mix_logits_before_weights true
# python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_addition_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_addition"  --self_condition_mix_logits_before_weights true
# python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_"${num_inference_diffusion_steps}"_self_condition_ablations/logits_multiply_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100 --self_condition "logits_multiply"  --self_condition_mix_logits_before_weights true

######################################################
# python run_simplification.py --model_name_or_path roberta-large --do_train --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/debug/" --per_device_train_batch_size=12 --per_device_eval_batch_size=25 --overwrite_output_dir  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --max_eval_samples 100  --self_condition_mix_logits_before_weights true --self_condition "logits_mean"
######################################################

# Running the evals.
num_inference_diffusion_steps=1000
: '
python -m torch.distributed.launch --nproc_per_node 4  run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max/checkpoint-19000/"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_max" --load_states_in_eval_from_model_path true
python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean/checkpoint-19000"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --self_condition "logits_mean" --load_states_in_eval_from_model_path true
python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition/checkpoint-19000"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition" --per_device_train_batch_size=12 --per_device_eval_batch_size=25  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_addition" --load_states_in_eval_from_model_path true
python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply/checkpoint-18000"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_multiply" --load_states_in_eval_from_model_path true
'

: '
python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max_self_condition_mix_before_weights/checkpoint-18000" --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --self_condition "logits_max"  --self_condition_mix_before_weights true  --load_states_in_eval_from_model_path true

python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean_self_condition_mix_before_weights/checkpoint-17000"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_mean"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true

python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition_self_condition_mix_before_weights/checkpoint-18000/"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25  --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --self_condition "logits_addition"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true

python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply_self_condition_mix_before_weights/checkpoint-17000"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply_self_condition_mix_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_multiply"  --self_condition_mix_before_weights true --load_states_in_eval_from_model_path true
'

python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/no_self_condition/checkpoint-22000"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/no_self_condition" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --load_states_in_eval_from_model_path
python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max_self_condition_mix_logits_before_weights/checkpoint-14000"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_max_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}  --self_condition "logits_max"  --self_condition_mix_logits_before_weights true --load_states_in_eval_from_model_path true
python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean_self_condition_mix_logits_before_weights/checkpoint-14000"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_mean_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_mean"  --self_condition_mix_logits_before_weights true --load_states_in_eval_from_model_path true
python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition_self_condition_mix_logits_before_weights/checkpoint-14000" --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_addition_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_addition"  --self_condition_mix_logits_before_weights true --load_states_in_eval_from_model_path true
python -m torch.distributed.launch --nproc_per_node 4 run_simplification.py --model_name_or_path "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply_self_condition_mix_logits_before_weights/checkpoint-16000"  --do_eval --dataset_name wikilarge  --output_dir "/net/nfs.cirrascale/s2-research/rabeehk/outputs/paper_experiments/simplification_1000_self_condition_ablations/logits_multiply_self_condition_mix_logits_before_weights" --per_device_train_batch_size=12 --per_device_eval_batch_size=25   --report_to tensorboard --eval_steps 1000  --max_steps 1000000 --max_source_length 64  --max_target_length 64 --max_seq_length 128 --conditional_generation "seq2seq" --num_inference_diffusion_steps ${num_inference_diffusion_steps} --evaluation_strategy steps --simplex_value 5 --num_diffusion_steps 5000 --lr_scheduler_type linear --learning_rate 1e-4 --pad_to_max_length  --beta_schedule squaredcos_improved_ddpm --weight_decay 0.01 --top_p 0.99 --warmup_steps 2000 --logging_steps 50 --save_steps 1000 ${PARAMS_FOR_LOCAL}   --self_condition "logits_multiply"  --self_condition_mix_logits_before_weights true --load_states_in_eval_from_model_path true
